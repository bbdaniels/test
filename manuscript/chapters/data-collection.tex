%------------------------------------------------

\begin{fullwidth}
Most data collection is now done using digital data entry
using tools that are specially designed for surveys.
These tools, called ``computer-assisted personal interviewing'' (CAPI) softwares,
provide a wide range of features designed to make
implementing even highly complex surveys easy, scalable, and secure.
However, these are not fully automatic:
you still need to actively design and manage the survey.
Each software has specific practices that you need to follow
to enable features such as Stata-compatibility and data encryption.

While you can work in any software you like,
this guide will present tools and best practices
for working with SurveyCTO, a proprietary implementation of Open Data Kit (ODK).
Most of the processes below will also replicate in ODK
with minimal adjustment for the exact setup you have.
The important parts, of course, are primarily conceptual:
this chapter should provide a motivation for
planning data structure during survey design,
developing surveys that are easy to control for quality and security,
and having proper file storage ready for sensitive PII data.
\end{fullwidth}

%------------------------------------------------

\section{Designing questionnaires with SurveyCTO}

SurveyCTO surveys are primarily created in Excel or Google Sheets,
making them one of the few outputs for which no coding is required.
\marginnote{The \texttt{ietestform} command, part of
\texttt{iefieldkit}, implements form-checking routines
for some SurveyCTO best practices. You can find a summary
of those practices at \url{https://dimewiki.worldbank.org/wiki/ietestform}.}
However, since they make extensive use of logical structure and
relate directly to the data that will be used later,
both the field team and the data team should
collaborate to make sure that the survey suits all needs.\cite{krosnick2018questionnaire}

Generally, this collaboration means building the experimental design
fundamentally into the structure of the survey.
When ID matching and tracking is essential,
the survey should be prepared to verify against preloads
of data from master records or from other rounds,
and ``extensive tracking'' sections --
in which reasons for attrition, treatment contamination, and
loss to follow-up can be documented --
are essential data components for completing CONSORT records.\cite{begg1996improving}

The survey design is the first part where the data team
and the field team must collaborate on data structure.
The field-oriented staff and the PIs will likely prefer
to capture a large amount of detailed \texttt{information}
in the field, some of which will serve very poorly as \texttt{data}.\sidenote{\url{
https://iriss.stanford.edu/sites/g/files/sbiybj6196/f/questionnaire\_design\_1.pdf}}
In particular, open-ended responses and questions which will have
many null or missing responses by design will not be very useful
in statistical analyses unless pre-planned.
You must work with the field team to determine the appropriate amount
of abstraction inherent in linking concepts to responses.\sidenote{\url{
https://www.povertyactionlab.org/sites/default/files/documents/Instrument\%20Design\_Diva\_final.pdf}}
It is always possible to ask for specific responses to questions,
but it is far more useful to ask for things like Likert responses.

Coded responses are always more useful than open-ended responses,
because they reduce the time necessary for post-processing by
expensive specialized staff.
For example, if collecting data on medication use or supplies,
you could collect: the brand name of the product;
the generic name of the product;
the ATC-coded compound of the product;
or the broad category to which each product belongs (antibiotic, etc.).
All four may be useful for different reasons,
but the latter two are likely to be the most useful for the analyst.
The ATC-coded compound requires providing a translation dictionary
to field staff, but enables automated rapid recoding for analysis
with no loss of information.
The generic class requires agreement on the broad categories of interest,
but allow for much more comprehensible top-line statistics and data quality checks.

Broadly, the questionnaire should be designed as follows.
The workflow will feel much like writing an essay:
we begin from broad concepts and slowly flesh them out to specifics.
\marginnote{Modules should not be numbered --
they should use a short prefix so they can be easily reordered.
There is not yet a full consensus over how individual questions should be identified:
formats like \texttt{hq\_1} are hard to remember and unpleasant to reorder,
but formats like \texttt{hq\_asked\_about\_loans} quickly become too long.}
The theory of change, experimental design, and pre-analysis plans should be discussed
and the structure of required data conceptualized.
All the conceptual outcomes of interest, as well as the covariates, classifications,
and other variables needed for the experimental design should be listed out.
The questionnaire \textit{modules} should be outlined based on this list.
Each module should then be expanded into specific indicators to observe in the field.
Finally, the questionnaire can be piloted in a non-experimental sample.
Revisions are made, and the survey is then translated into the appropriate language and programmed electronically.

%------------------------------------------------

\section{Collecting data securely}

Any established data collection service will always encrypt
all data submitted from the field automatically while in transit
(ie, upload or download), so if you use servers hosted by SurveyCTO
this is nothing you need to worry about.
Your data will be encrypted from the time it leaves the device
(in tablet-assisted data collation) or your browser (in web data collection)
until it reaches the server.

\marginnote{Encryption at rest is the only way to ensure
that PII data remains private when it is stored
on someone else’s server on the open internet.
The World Bank’s and many of our donors’ security requirements
for data storage can only be fulfilled by this method.
We recommend keeping your data encrypted whenever PII data is collected --
therefore, we recommend it for all field data collection.}

Encryption in cloud storage, by contrast, is not enabled by default.
This is because the service will not encrypt user data unless you confirm
you know how to operate the encryption system and assume its risks.
Encryption at rest is different from password-protection:
encryption at rest makes the underlying data itself unreadable,
even if accessed, except to users who have a specific private key file.
Encryption at rest requires active participation from you, the user,
and you should be fully aware that if your private key is lost,
there is absolutely no way to recover your data.

To enable data encryption, you simply select the encryption option
when you create a new form on a SurveyCTO server.
At that time, the service will allow you to download -- once --
the keyfile pair needed to decrypt the data.
You must download and store this in a secure location.
Again, we recommend LastPass, a software whose free option
allows you to store passwords as well as small keyfiles like this.
You should make sure the ``secure note'' which you put the keyfile in
is descriptively named to match the survey to which it corresponds.
The Sync app will ask you for the location of this file
when you download and set up data for use,
and all you will need to do is copy that file to your desktop,
point Sync to it, and the rest is automatic.

Finally, you should ensure that all teams take basic precautions
to ensure the security of data, as most problems are due to human error.
Most importantly, all computers, tablets, and accounts used
\textit{must} have a logon password associated with them.
This policy should also be applied to physical data storage
such as flash drives and hard drives;
similarly, files sent to the field containing PII data
such as the sampling list should at least be password-protected.\sidenote{
This can be done using a zip-file creator.
LastPass can also be used to share passwords securely,
and you cannot share passwords across email.}
This step significantly mitigates the risk in case there is
a security breach such as loss, theft, hacking, or a virus,
and adds very little hassle to utilization.


%------------------------------------------------

\section{Overseeing fieldwork and quality assurance}

While the team is in the field, you will be responsible
for making sure that the survey is progressing correctly,
that the collected data matches the survey sample,
and that errors and duplicate observations are resolved
quickly so that the field team can make corrections.
Modern survey software makes it relatively easy
to control for issues in individual surveys,
using a combination of in-built features
such as hard constraints on answer ranges
and soft confirmations or validation questions.

These features mean you can spend more time
looking for issues that the software cannot check.
Namely, these will be outliers from the overall distribution
or a group of surveys rather than errors in any one survey:
enumerators who are taking too long to complete their work,
``difficult'' groups of respondents who are systematically incomplete,
and checks for systematic response errors and outliers.
These are typically done in two main forms:
high-frequency checks (HFCs) and back-checks.

High-frequency checks are carried out on the data
side.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Monitoring_Data_Quality}}
First, observations must be validated against the sample list:
this is as straightforward as conducting a \texttt{merge}
of the survey data and checking for mismatches.
Next, observations need to be checked for duplicate entries:
\texttt{ieduplicates}\sidenote{\url{https://dimewiki.worldbank.org/wiki/ieduplicates}}
provides a workflow for collaborating on the resolution of
duplicate entries between you and the field team
Finally, data quality checks\sidenote{\url{https://dimewiki.worldbank.org/wiki/Monitoring_Data_Quality}}
should be run on the data every time it is downloaded
to flag irregularities in observations, sample groups, or enumerators.

Unfortunately, it is very hard to specify in general
what kinds of quality checks should be utilized,
since the content of surveys varies so widely.
Fortunately, you will know your survey very well
by the time it is programmed, and should have a good sense
of the types of things that would raise concerns
that you were unable to program directly into the survey.
Thankfully it is also easy to prepare high-frequency checking code in advance.
Once you have built and piloted the survey form,
you will have some bits of mock data to play with.
Using this data, you should prepare code that outputs
a list of flags from any given dataset.
This HFC code is then ready to run every time you download the data,
and should allow you to rapidly identify issues
that are occuring in fieldwork.

Back-checks\sidenote{\url{https://dimewiki.worldbank.org/wiki/Back_Checks}}
involve more extensive collaboration with the field team,
and are best thought of as direct data audits.
In back-checks, a random subset of the field sample is chosen
and basic information from the full survey is verified
using a small survey re-done with the same respondent.
Back-checks should be done with a substantial portion
of the full sample early in the survey,
so that the enumerators and field team
get used to the idea of quality assurance.
Checks should continue throughout fieldwork,
and their content and targeting can be refined if particular
questionnaire items are flagged as error-prone
or specific enumerators or observations appear unusual.

Back-checks cover three main types of questions.
First, they validate basic information that should not change.
This ensures the basic quality control that the right respondent
was interviewed or observed in a given survey,
and flags cases of outright quality failure that need action.
Second, they check the quality of enumeration,
particularly in cases that involve measurement or calculation
on the part of the enumerator.
This can be anything such as correctly calcuting plot sizes,
family rosters, or income measurements.
These questions should be carefully validated
to determine whether they are reliable measures
and how much they may vary as a result of the difficulty in measurement.
Finally, back-checks confirm that questions are being asked and answered
in a consistent fashion. Some questions, if poorly phrased,
can be hard for the eumerator to express or for all respondents
to understand in an identical fashion,
and changes in these questions between two surveys of the same respondent
can alert you and the team that changes need to be made
to the execution of portions of the survey.

When all data collection is complete,
the survey team should have a final field report
ready for validation against the sample frame and the dataset.
This should contain all the observations that were completed;
it should merge perfectly with the received dataset;
and it should report reasons for any observations not collected.
Reporting of missingness and attrition is critical
to the interpretation of any survey dataset,
so it is important to structure this reporting
such that broad rationales can be grouped into categories
but also that the field team can provide detailed, open-ended responses
for any observations they were unable to complete.
This reporting should be validated and saved
alongside the final raw data, and treated the same way.


%------------------------------------------------

\section{Receiving primary data}

In this section, you finally get your hands on some data!
\marginnote{The \texttt{iefolder} command from \texttt{ietoolkit}
will be your best friend here. This command will set up
the folder structure for all data processing tasks
across all data collection rounds.
\url{https://dimewiki.worldbank.org/iefolder}}
What do we do with it? Data handling is one of the biggest
``black boxes'' in primary research -- it always gets done,
but teams have wildly different approaches for actually doing it.
This section breaks the process into key conceptual steps
and provides at least one practical solution for each.
Initial receipt of data will proceed as follows:
the data will be downloaded, and a ``gold master'' copy
of the raw data should be permanently stored in a secure location.
Then, a ``master'' copy of the data is placed into an encrypted location
that will remain accessible on disk and backed up.
This handling satisfies the rule of three:
there are two on-site copies of the data and one off-site copy,
so the data can never be lost in case of hardware failure.

For this step, the remote location can be a variety of forms:
the cheapest is a long-term cloud storage service
such as Amazon Web Services or Microsoft Azure.
Equally sufficient is a physical hard drive
stored somewhere other than the primary work location.
Enterprise cloud solutions like Microsoft OneDrive
can also work, although because you do not control them
we typically do not recommend this for the gold master copy.
If the service satisfies your security needs,
the raw data can be stored unencrypted here.
If you remain lucky, you will never have to access this copy --
you just want to know it is out there, safe, if you need it.

The copy of the raw data you are going to use
should be handled with care.
Since you will probably need to share it among the team,
it should be placed in an encrypted storage location.
The combination of Dropbox and VeraCrypt
can satisfy this requirement for some teams,
since this will never make the data visible to someone
who gets access to the Dropbox,
without the key to the file that is generated on encryption.
\marginnote{LastPass or equivalent should be used to store and share keys.
Note also that this data copy can never be considered the gold master,
because other users or the service can accidentally delete it.}
\textit{The file must never be placed in Dropbox unencrypted, however.}
The way VeraCrypt works is that it creates a virtual copy
of the unencrypted file outside of Dropbox, and lets you access that copy.
Since you should never edit the raw data, this will not be very cumbersome,
but the unencryption and usage of the raw data is a manual process.

You should only interact with the raw master data a couple of times.
The first task is therefore to create a derived copy --
a \textbf{de-identified} copy --
which you can freely share to team members on Dropbox unencrypted.
This dataset is produced by removing personally-identifying fields
from the raw master dataset. Nothing else is altered.
This step is as easy as dropping variables,
and since it only affects survey fields,
the variables to be kept can be indicated at the time the survey is written.
There is no harm in being overly conservative at this stage,
since if you find that another field is required,
you can re-access the encrypted master data and add it to the dataset.
With the unencrypted, de-identified data in a shared location
and the raw data securely stored and backed up,
you are ready to move to data cleaning and analysis.
